{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ae381387ab4bc3af9e7a16b054bdb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Model Type:', index=2, options=('FNDNet', 'HAN', 'BERT'), value='BERT'), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b25c564ff724ada8a423ff1a2288535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Textarea(value=' ', description='Title:', layout=Layout(height='30px', width='70…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96abc1bca852489ea4ec886ff0400446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid black', width='700px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import yaml\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from dataset import create_tokenizer\n",
    "from models import create_model\n",
    "\n",
    "from captum.attr import TokenReferenceBase, LayerIntegratedGradients\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def _get_mask(start_idx, input_ids, sent_ids):\n",
    "    \n",
    "    mask = torch.zeros_like(input_ids).to(torch.bool)\n",
    "    \n",
    "    if len(input_ids[start_idx:]) < len(sent_ids):\n",
    "        mask[start_idx:] = True\n",
    "        return mask\n",
    "    \n",
    "    for i in range(start_idx, len(input_ids) - len(sent_ids)):    \n",
    "        match = input_ids[i:i+len(sent_ids)] == sent_ids\n",
    "\n",
    "        if match.sum() == len(sent_ids):        \n",
    "            mask[i:i+len(sent_ids)] = True\n",
    "            break\n",
    "\n",
    "    return mask\n",
    "\n",
    "def get_sentences_mask(input_ids, sents, encode_func):\n",
    "    sents_mask = []\n",
    "    start_idx = 0\n",
    "    for sent in sents:\n",
    "\n",
    "        mask = _get_mask(\n",
    "            start_idx = start_idx,\n",
    "            input_ids = input_ids, \n",
    "            sent_ids  = torch.tensor(encode_func(sent))\n",
    "        )\n",
    "\n",
    "        sents_mask.append(mask)\n",
    "\n",
    "        check = torch.where(mask == True)[0]\n",
    "        if check.sum() == 0:\n",
    "            break\n",
    "        else:\n",
    "            start_idx = torch.where(mask == True)[0][-1] + 1\n",
    "        \n",
    "    return sents_mask\n",
    "\n",
    "def print_sent_with_score(sent, score):\n",
    "    cmap = plt.cm.get_cmap('Greens')\n",
    "    \n",
    "    # calc score of sentence\n",
    "    temp = np.array(cmap(score))\n",
    "    temp[:3] = (temp[:3] * 255).astype(np.uint8)\n",
    "    temp[-1] *= 0.8\n",
    "    \n",
    "    return widgets.HTML(f'<span class=\"barcode\"; style=\"color: black; background-color: rgba{tuple(temp)}\">{sent}</span>')\n",
    "\n",
    "\n",
    "output_model = widgets.Output()\n",
    "\n",
    "@output_model.capture()\n",
    "def on_click_model(b: widgets.Button) -> None:\n",
    "    global model\n",
    "    global dataset\n",
    "    global tokenizer\n",
    "    global cfg\n",
    "    \n",
    "    modelname = {\n",
    "        'BERT':'BERT',\n",
    "        'FNDNet':'FNDNet',\n",
    "        'HAND':'HAND'\n",
    "    }\n",
    "    \n",
    "    cfg = yaml.load(\n",
    "        open(f'./configs/{model_select.value}/{modelname[model_select.value]}-test.yaml','r'), \n",
    "        Loader = yaml.FullLoader\n",
    "    )\n",
    "\n",
    "    tokenizer, word_embed = create_tokenizer(\n",
    "        name            = cfg['TOKENIZER']['name'], \n",
    "        vocab_path      = cfg['TOKENIZER'].get('vocab_path', None), \n",
    "        max_vocab_size  = cfg['TOKENIZER'].get('max_vocab_size', None)\n",
    "    )\n",
    "\n",
    "    model = create_model(\n",
    "        modelname                 = cfg['MODEL']['modelname'],\n",
    "        hparams                   = cfg['MODEL']['PARAMETERS'],\n",
    "        word_embed                = word_embed,\n",
    "        tokenizer                 = tokenizer,\n",
    "        freeze_word_embed         = cfg['MODEL'].get('freeze_word_embed',False),\n",
    "        use_pretrained_word_embed = cfg['MODEL'].get('use_pretrained_word_embed',False),\n",
    "        checkpoint_path           = cfg['MODEL']['CHECKPOINT']['checkpoint_path'],\n",
    "    )\n",
    "    model.eval()\n",
    "    \n",
    "    dataset = __import__('dataset').__dict__[f'{cfg[\"DATASET\"][\"name\"]}Dataset'](\n",
    "        tokenizer = tokenizer,\n",
    "        **cfg[\"DATASET\"]['PARAMETERS']\n",
    "    )\n",
    "    \n",
    "    \n",
    "output = widgets.Output(layout=widgets.Layout(width='700px', border='1px solid black'))\n",
    "\n",
    "@output.capture()\n",
    "def on_click_run(b: widgets.Button) -> None:\n",
    "    # transform inputs\n",
    "    inputs = dataset.transform(\n",
    "        title = title.value,\n",
    "        text  = text.value.split('\\\\n')\n",
    "    )\n",
    "\n",
    "    # prediction\n",
    "    outputs = model(**dict([(k,v.unsqueeze(0)) for k,v in inputs.items()])).detach()[0]\n",
    "    outputs = torch.nn.functional.softmax(outputs, dim=-1)\n",
    "    pred = outputs.argmax(dim=-1)\n",
    "\n",
    "    if model_select.value == 'HAND':\n",
    "        sents_score = model(\n",
    "            **dict([(k,v.unsqueeze(0)) for k,v in inputs.items()]),\n",
    "            output_attentions = True\n",
    "        )[2].detach()[0]\n",
    "\n",
    "    else:\n",
    "        # define word embedding layer\n",
    "        if model_select.value == 'BERT':\n",
    "            def bert_encode(src):\n",
    "                return [tokenizer.convert_tokens_to_ids(s) for s in tokenizer(src)]\n",
    "            \n",
    "            layer = model.bert.embeddings.word_embeddings\n",
    "            refer_token_idx = tokenizer.vocab.token_to_idx['PAD']\n",
    "            encode_func = bert_encode\n",
    "            \n",
    "        elif model_select.value == 'FNDNet':\n",
    "            layer = model.w2e\n",
    "            refer_token_idx = tokenizer.pad_token_id\n",
    "            encode_func = tokenizer.encode\n",
    "\n",
    "        # calc attribution\n",
    "        token_reference = TokenReferenceBase(reference_token_idx=refer_token_idx)\n",
    "        attr = LayerIntegratedGradients(model, layer)\n",
    "\n",
    "        reference_indices = token_reference.generate_reference(\n",
    "            sequence_length = cfg['DATASET']['PARAMETERS']['max_word_len'], \n",
    "            device          = 'cpu'\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        attr_score, delta = attr.attribute(\n",
    "            inputs                   = inputs['input_ids'].unsqueeze(0), \n",
    "            baselines                = reference_indices, \n",
    "            target                   = pred, \n",
    "            n_steps                  = 10, \n",
    "            return_convergence_delta = True\n",
    "        )\n",
    "\n",
    "        attr_score = attr_score.squeeze().sum(dim=-1)\n",
    "\n",
    "\n",
    "        # sentences mask\n",
    "        sents_mask = get_sentences_mask(\n",
    "            input_ids = inputs['input_ids'],\n",
    "            sents = text.value.split('\\\\n'),\n",
    "            encode_func = encode_func\n",
    "        )\n",
    "\n",
    "\n",
    "        # get attribution score per sentence\n",
    "        sents_score = []\n",
    "\n",
    "        for mask in sents_mask:\n",
    "            score = attr_score[mask]\n",
    "\n",
    "            # calculate only positive scores\n",
    "            pos_mask = score > 0\n",
    "            score = score[pos_mask].sum().item()\n",
    "\n",
    "            sents_score.append(score)\n",
    "\n",
    "    # scaling\n",
    "    sents_score = np.array(sents_score)\n",
    "    sents_score /= sents_score.sum()\n",
    "\n",
    "\n",
    "    # print result\n",
    "    cls = ['real','fake']\n",
    "    \n",
    "    html_list = []\n",
    "    \n",
    "    html_list.append(widgets.HTML('<h1>Result</h1>'))\n",
    "    html_list.append(widgets.HTML(f\"{cls[pred].capitalize()} ( {outputs[pred]:.2%} )\"))\n",
    "\n",
    "    \n",
    "    html_list.append(widgets.HTML('<h1>Title</h1>'))\n",
    "\n",
    "    html_list.append(widgets.HTML(f'<span class=\"barcode\"; style=\"color: black;\">{title.value}</span>'))\n",
    "\n",
    "    html_list.append(widgets.HTML('<h1>Context</h1>'))\n",
    "\n",
    "    for sent, score in zip(text.value.split('\\\\n'), sents_score):\n",
    "        html_list.append(print_sent_with_score(sent=sent, score=score))\n",
    "            \n",
    "    display(widgets.VBox(html_list))\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    \n",
    "# =====================\n",
    "# layout\n",
    "# =====================\n",
    "\n",
    "model_select = widgets.Dropdown(\n",
    "    options=['FNDNet','HAND','BERT'],\n",
    "    value='BERT',\n",
    "    description='Model Type:',\n",
    "    disabled=False,\n",
    ")\n",
    "    \n",
    "title = widgets.Textarea(\n",
    "    value=' ',\n",
    "    placeholder='',\n",
    "    description='Title:',\n",
    "    layout = widgets.Layout(width='700px', height='30px')\n",
    ")\n",
    "\n",
    "text = widgets.Textarea(\n",
    "    value=' ',\n",
    "    placeholder='',\n",
    "    description='Context:',\n",
    "    layout = widgets.Layout(width='700px', height='500px')\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "button_modelname = widgets.Button(description='select')\n",
    "button_modelname.on_click(on_click_model)\n",
    "\n",
    "button_run = widgets.Button(description='run')\n",
    "button_run.on_click(on_click_run)\n",
    "\n",
    "display(widgets.HBox([model_select, button_modelname]))\n",
    "\n",
    "display(widgets.VBox([widgets.HBox([title,button_run]), text]))\n",
    "\n",
    "display(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
